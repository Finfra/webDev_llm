# LM Studio 설치 및 테스트

## 1. LM Studio 소개

### LM Studio란?
* LM Studio는 오픈소스 LLM(Large Language Model)을 로컬 환경에서 쉽게 실행할 수 있게 해주는 데스크톱 애플리케이션
* 대규모 언어 모델을 다운로드하고 개인 컴퓨터에서 직접 실행 가능
* 인터넷 연결 없이 프라이빗하게 AI 모델 사용 가능
* OpenAI API와 호환되는 로컬 서버를 제공해 다양한 애플리케이션과 연동 가능

### 주요 특징 및 장점
* 로컬 실행 - 데이터가 외부로 전송되지 않아 프라이버시 보장
* 다양한 오픈소스 모델 지원 - Llama, Mistral, Vicuna 등 다양한 모델 사용 가능
* 사용자 친화적 인터페이스 - 코딩 지식 없이도 쉽게 사용 가능
* OpenAI 호환 API - 기존 OpenAI API 기반 앱과 쉽게 통합 가능
* 크로스 플랫폼 지원 - Windows, macOS, Linux 지원
* 모델 파라미터 조정 가능 - 다양한 설정으로 모델 동작 최적화 가능
* 무료 사용 - 별도의 구독료 없이 무료로 사용 가능

### 사용 사례
* 개인 AI 어시스턴트 - 오프라인 환경에서 작동하는 개인 비서
* 개발자 도구 - 코드 작성, 디버깅, 문서화 지원
* 콘텐츠 생성 - 블로그 포스트, 마케팅 텍스트, 창의적 글쓰기
* 데이터 프라이버시가 중요한 작업 - 민감한 정보를 외부 API에 노출하지 않고 처리
* 교육 목적 - AI 및 LLM 기술 학습 및 실험
* API 통합 - 자체 애플리케이션 개발 시 AI 기능 내장

## 2. 설치 방법

### 시스템 요구사항
* CPU: 최소 4코어 이상 (8코어 이상 권장)
* RAM: 최소 8GB (16GB 이상 권장)
* 저장공간: 최소 10GB (모델 크기에 따라 20GB 이상 필요할 수 있음)
* GPU: 
  - NVIDIA GPU: CUDA 지원 가능한 GPU (최소 4GB VRAM, 8GB 이상 권장)
  - Apple Silicon: M1/M2/M3 시리즈 통합 GPU 지원
* 운영체제:
  - Windows 10/11 (64비트)
  - macOS 11+ (Big Sur 이상)
  - Ubuntu 20.04 이상

### 다운로드 방법
* 공식 웹사이트
  - LM Studio 공식 웹사이트 (https://lmstudio.ai) 방문
  - 메인 페이지에서 "Download" 버튼 클릭
  - 운영체제에 맞는 버전 선택하여 다운로드

* 지원 플랫폼
  - Windows: .exe 설치 파일
  - macOS: .dmg 설치 파일 (Intel 및 Apple Silicon 버전 별도)
  - Linux: .AppImage 또는 .deb 패키지

### 설치 과정
* Windows 설치 가이드
  - 다운로드한 .exe 파일 실행
  - 설치 마법사의 안내에 따라 진행
  - 설치 위치 선택 및 추가 옵션 설정
  - 설치 완료 후 바탕화면 또는 시작 메뉴에서 실행

* macOS 설치 가이드
  - 다운로드한 .dmg 파일 실행
  - LM Studio 아이콘을 Applications 폴더로 드래그
  - 첫 실행 시 "확인되지 않은 개발자" 경고가 표시될 경우:
    - 시스템 환경설정 > 보안 및 개인 정보 보호로 이동
    - "앱 다운로드 허용" 섹션에서 "열기" 버튼 클릭

* Linux 설치 가이드
  - .AppImage 파일 사용 시:
    - 다운로드한 파일에 실행 권한 부여: `chmod +x LMStudio-*.AppImage`
    - AppImage 파일 실행: `./LMStudio-*.AppImage`
  - .deb 패키지 사용 시:
    - 터미널에서 설치: `sudo dpkg -i LMStudio-*.deb`
    - 의존성 문제 해결: `sudo apt-get install -f`

## 3. LM Studio 초기 설정

### 언어 및 환경 설정
* 첫 실행 시 언어 선택 가능 (기본값: 영어)
* 설정 메뉴(Settings)에서 다음 항목 구성 가능:
  - 인터페이스 언어 변경
  - 다크/라이트 테마 선택
  - GPU 가속 활성화/비활성화
  - 모델 저장 위치 지정
* 설정은 앱 우측 하단의 '설정' 아이콘을 통해 접근 가능

### 모델 다운로드 방법
* 왼쪽 메뉴에서 'Models' 탭 선택
* 'Browse Models' 클릭하여 모델 라이브러리 접근
* 원하는 모델 검색 (ex: "mistral", "llama", "phi")
* 모델 크기와 성능 정보 확인 후 다운로드 버튼 클릭
* 다운로드 진행 상황은 하단 상태 바에서 확인 가능
* 다운로드 완료 후 모델 목록에 표시됨

### 인터페이스 둘러보기
* 주요 인터페이스 구성요소:
  - 좌측 패널: 모델 관리, 채팅 세션, 설정 메뉴
  - 중앙 영역: 채팅 인터페이스 또는 모델 브라우저
  - 우측 패널: 모델 파라미터 및 생성 설정
* 하단 상태 표시줄:
  - GPU/CPU 사용량
  - 메모리 사용 정보
  - 현재 선택된 모델 정보
* 모델을 선택하면 채팅 인터페이스로 자동 전환됨

## 4. 로컬 모델 설치 및 실행

### 지원 모델 목록
* 주요 지원 모델 계열:
  - Llama 2 및 Llama 3 계열
  - Mistral 및 Mixtral 계열
  - Vicuna 모델
  - Phi 모델
  - Falcon 모델
  - MPT 모델
  - Pythia 모델
* 모델 파일 형식:
  - GGUF (.gguf) - 가장 많이 지원되는 형식
  - GGML (.ggml) - 구형 모델 형식
  - 기타 호환 형식

### 모델 다운로드 방법
* 내장 브라우저를 통한 다운로드:
  - Models 탭 > Browse 클릭
  - 카테고리별 필터링 또는 검색 기능 사용
  - 모델 선택 후 다운로드 버튼 클릭
* 외부에서 다운로드한 모델 추가:
  - HuggingFace 또는 다른 소스에서 GGUF/GGML 모델 파일 다운로드
  - Models 탭 > Import 버튼 클릭
  - 다운로드한 모델 파일 선택

### 모델 불러오기 및 설정
* 모델 목록에서 사용할 모델 선택
* "Load" 버튼 클릭하여 메모리에 로드
* 우측 패널에서 추론 설정 조정:
  - Temperature: 응답의 창의성/무작위성 조절 (0.1-2.0)
  - Top P: 토큰 샘플링의 다양성 조절 (0.1-1.0)
  - Top K: 고려할 다음 토큰 개수 제한
  - Max Tokens: 생성할 최대 토큰 수 설정
  - Prompt Format: 모델에 적합한 프롬프트 형식 선택
* 고급 설정:
  - Context Length: 모델이 기억할 컨텍스트 길이 설정
  - Layers: GPU 메모리에 로드할 레이어 수 (성능/메모리 트레이드오프)

## 5. 채팅 인터페이스 사용법

### 새 채팅 시작하기
* 좌측 패널에서 "New Chat" 버튼 클릭
* 새 채팅 세션이 생성되고 자동으로 선택됨
* 채팅명은 자동 생성되지만 우클릭하여 변경 가능
* 채팅을 폴더로 구성하려면 "New Folder" 생성 후 채팅을 폴더로 드래그

### 프롬프트 설정하기
* 채팅 시작 전 모델의 행동을 정의하는 시스템 프롬프트 설정 가능
* 시스템 프롬프트 설정 방법:
  - 우측 상단의 "System Prompt" 버튼 클릭
  - 모델의 역할과 동작 방식 정의
  - ex) "당신은 프로그래밍을 도와주는 AI 어시스턴트입니다."
* 채팅 입력창에 메시지 작성 후 엔터 또는 전송 버튼 클릭
* 모델 응답이 실시간으로 표시됨

### 채팅 기록 관리
* 모든 채팅 세션은 자동 저장됨
* 채팅 세션 관리:
  - 이름 변경: 채팅 세션 우클릭 > Rename
  - 삭제: 채팅 세션 우클릭 > Delete
  - 복제: 채팅 세션 우클릭 > Duplicate
* 히스토리 내보내기/가져오기:
  - 내보내기: 채팅 세션 우클릭 > Export
  - 가져오기: 좌측 패널 하단의 Import 버튼
* 채팅 내 메시지 관리:
  - 개별 메시지 수정: 메시지 우클릭 > Edit
  - 메시지 삭제: 메시지 우클릭 > Delete
  - 대화 중간부터 재생성: 특정 메시지 후 "Regenerate" 버튼 클릭

## 6. API 서버 설정 및 활용

### API 서버 시작하기
* API 기능 활성화:
  - 상단 메뉴바에서 "Local Server" 탭 선택
  - "Start Local Server" 버튼 클릭
* 서버 상태 확인:
  - 녹색 표시등은 서버가 실행 중임을 나타냄
  - 현재 서버 URL 표시 (기본: http://localhost:1234)
* 서버 설정 조정:
  - 포트 번호 변경 (기본값: 1234)
  - 인증 토큰 설정 (선택사항)
  - 로그 레벨 설정

### API 엔드포인트 구성
* 기본 API 엔드포인트:
  - `/v1/chat/completions`: 채팅 완성 API
  - `/v1/completions`: 텍스트 완성 API
  - `/v1/models`: 사용 가능한 모델 목록
* API 요청 형식:
  - OpenAI API와 호환되는 JSON 형식 사용
  - 필수 파라미터: model, messages (채팅 API의 경우)
  - 선택적 파라미터: temperature, max_tokens, stream 등
* API 테스트:
  - 내장 테스트 콘솔 사용 가능
  - cURL 또는 Postman 등의 외부 도구로 테스트 가능

### OpenAI 호환 API 활용 방법
* OpenAI SDK와 호환:
  - OpenAI 클라이언트 라이브러리 사용 가능
  - base_url 파라미터만 LM Studio URL로 변경
* 다양한 언어에서 API 호출:
  - Python, JavaScript, Go, Ruby 등 지원
* 인증 토큰 사용:
  - 토큰을 설정한 경우 API 요청 헤더에 포함
  - `Authorization: Bearer your_token_here`
* 스트리밍 응답 지원:
  - stream=true 파라미터로 실시간 응답 수신 가능
* 오류 처리:
  - OpenAI API와 동일한 오류 형식 사용
  - HTTP 상태 코드 및 오류 메시지 확인

## 7. 고급 설정 및 최적화

### GPU 가속 설정
* GPU 설정 접근:
  - 설정 메뉴 > GPU 탭
* GPU 가속 활성화:
  - "Enable GPU acceleration" 체크박스 활성화
  - 여러 GPU가 있는 경우 사용할 GPU 선택 가능
* 메모리 할당:
  - 모델 로드에 사용할 GPU 메모리 양 설정
  - 모델 크기에 따라 자동 또는 수동 설정 가능
* 무거운 모델을 위한 설정:
  - GPU 스플리팅: 여러 GPU에 모델 분산 로드
  - CPU 오프로딩: 일부 레이어를 CPU에 로드

### 성능 최적화 팁
* 모델 크기 선택:
  - 작은 모델 (7B 파라미터) - 빠른 응답, 적은 리소스
  - 중간 모델 (13B 파라미터) - 균형잡힌 성능
  - 큰 모델 (70B 파라미터) - 더 나은 품질, 더 많은 리소스
* 양자화 수준 선택:
  - Q4_K_M - 메모리 효율성과 품질 균형
  - Q5_K_M - 더 나은 품질, 더 많은 메모리
  - Q8_0 - 최상의 품질, 가장 많은 메모리
* 추론 속도 개선:
  - 낮은 컨텍스트 길이 설정
  - 배치 크기 조정
  - 불필요한 백그라운드 앱 종료

### 메모리 관리 방법
* RAM 사용량 최적화:
  - 메모리 사용량이 적은 양자화 모델 선택 (Q4_K_M)
  - 작은 모델 (7B) 사용 고려
  - 불필요한 동시 실행 모델 언로드
* VRAM 관리:
  - GPU 레이어 수 조정 (전체 레이어의 일부만 GPU에 로드)
  - 필요한 경우에만 컨텍스트 길이 증가
  - 사용하지 않는 모델 언로드
* 모델 캐싱:
  - 자주 사용하는 모델을 위한 디스크 캐시 활성화
  - 메모리 매핑 옵션 활성화

## 8. 트러블슈팅

### 일반적인 오류 해결법
* 모델 로딩 실패:
  - 메모리 부족 오류: 더 작은 모델 또는 양자화 수준 선택
  - 파일 손상: 모델 재다운로드
  - 호환성 문제: 최신 버전의 LM Studio로 업데이트
* 응답 생성 중단:
  - 응답 길이 제한 확인
  - 컨텍스트 길이 초과 여부 확인
  - 메모리 부족 문제 확인
* API 연결 오류:
  - 로컬 서버 실행 여부 확인
  - 포트 충돌 확인 (다른 포트로 변경)
  - 방화벽 설정 검토

### 성능 이슈 해결
* 느린 응답 속도:
  - 더 작은 모델로 전환
  - 컨텍스트 길이 줄이기
  - Temperature 값 낮추기
  - GPU 메모리 할당량 증가
* 메모리 부족 문제:
  - 다른 애플리케이션 종료
  - 작은 모델 선택
  - 더 높은 양자화 수준 사용 (Q4 대신 Q3)
  - GPU 레이어 수 줄이기
* 품질 저하 문제:
  - 양자화 수준 높이기
  - 더 큰 모델 사용
  - 시스템 프롬프트 개선
  - Temperature 값 조정

### 리소스 사용량 최적화
* CPU 사용량 모니터링:
  - 작업 관리자/활동 모니터링에서 CPU 사용량 확인
  - 동시에 실행 중인 다른 CPU 집약적 작업 제한
* RAM 사용량 관리:
  - 메모리 매핑 사용 고려
  - 불필요한 모델 언로드
* 디스크 공간 관리:
  - 사용하지 않는 모델 삭제
  - 모델 저장 위치 별도 드라이브로 변경
* 배터리 수명 연장 (노트북 사용자):
  - 전원 관리 설정 최적화
  - 사용하지 않을 때 모델 언로드
  - 작은 모델 사용 고려

## 9. 실습1: 간단한 채팅 시나리오

* 되는 것
  - 일반적인 질문-답변 상호작용
  - 코드 작성 및 설명
  - 텍스트 요약 및 분석
  - 창의적인 글쓰기 및 아이디어 브레인스토밍
  - 간단한 수학 문제 해결
  - 지식 기반 질문 (모델의 훈련 데이터 범위 내)
  - 다양한 형식의 콘텐츠 생성 (에세이, 시, 스크립트 등)
  - 멀티턴 대화 유지

* 않되는 것
  - 실시간 정보 접근 (인터넷 연결 없음)
  - 모델의 훈련 데이터 이후 발생한 이벤트 인식
  - 이미지, 오디오, 비디오 처리
  - 파일 분석 (직접적인 파일 업로드 불가)
  - 웹 검색 또는 외부 데이터베이스 쿼리
  - 실제 시스템 접근 또는 파일 수정
  - 매우 긴 문서 처리 (컨텍스트 길이 제한)
  - 복잡한 수학적 계산이나 대규모 데이터 분석

## 10. 실습2: 모델 로딩 옵션 수정
* context 길이 : 각각 수정 후 사용되는 메모리량 확인
  - 컨텍스트 길이 설정 방법:
    - 모델 로드 시 우측 패널에서 "Context Length" 설정 조정
    - 기본값은 모델마다 다름 (대개 2048 또는 4096)
      - 단, 최소 11K이상은 셋팅해야. cline에서 겨우 작동하고, 32K 정도 되어야 원만히 작동함.
    - 필요에 따라 8K, 16K, 32K로 증가 가능
  - 긴 컨텍스트 설정 시 고려사항:
    - 메모리 사용량 크게 증가 (선형적으로 증가)
    - 처리 속도 감소
    - 모든 모델이 긴 컨텍스트를 효과적으로 활용하지 못할 수 있음
  - 적절한 컨텍스트 길이 선택:
    - 일반 대화: 2K-4K
    - 문서 분석: 8K-16K
    - 코드 작업: 8K-32K

* 로딩 메모리 줄이는 방법
  - 양자화 수준 조정:
    - 더 효율적인 양자화 선택 (Q4_K_M → Q3_K_M → Q2_K)
    - 품질과 메모리 사용량의 트레이드오프 고려
  - GPU 레이어 설정:
    - 모든 레이어를 GPU에 로드하지 않고 일부만 로드
    - "GPU Layers" 설정에서 레이어 수 조정
    - ex) 32개 레이어 모델에서 16개만 GPU에 로드
  - 메모리 매핑 활성화:
    - 설정에서 "Memory-map model files" 옵션 활성화
    - 필요한 부분만 메모리에 로드하여 RAM 사용량 감소
  - 불필요한 기능 비활성화:
    - KV 캐시 크기 제한
    - 병렬 토큰 생성 비활성화
    - 컨텍스트 길이 최소화
* CF) RoPE 기본 주파수 (RoPE Frequency Base) , RoPE 기본 스케일 (RoPE Frequency Scale) 실전 사용 팁
  • 대부분의 LLM은 기본값으로 충분 (RoPE base=10000, scale=1.0)
  • 긴 문맥 대응 모델을 로드할 경우:
  • base를 더 작게 (예: 2600~5000)
  • scale을 약간 조정 (ex: 0.9 ~ 1.2)


## 11. 실습3: 실습 간단한 채팅 시나리오
* 모델 로드 다운로드 되어 있어야 작동함.
* Node Js 코딩
  - 실행 방법:
    ```bash
    # 먼저 axios 패키지 설치
    npm install axios
    
    # 스크립트 실행
    node #아래 호출 예제 붙여넣기
    ```
  - 간단한 LM Studio API 호출 예제:
    ```javascript
    // lm-studio-test.js
    const axios = require('axios');
    
    async function askLMStudio() {
      try {
        // LM Studio 서버 호출 (기본 포트 1234)
        const response = await axios.post('http://localhost:1234/v1/chat/completions', {
          model: 'bitnet-m7-70m', // 현재 로드된 모델 이름
          messages: [
            { role: 'user', content: 'hi' }
          ],
          temperature: 0.7,
          max_tokens: 500
        });
        
        // 응답 출력
        console.log('AI 응답:');
        console.log(response.data.choices[0].message.content);
      } catch (error) {
        console.error('오류 발생:', error.message);
        console.log('LM Studio가 실행 중이고 모델이 로드되어 있는지 확인하세요.');
      }
    }
    
    // 함수 실행
    askLMStudio();
    ```
